# Environment Variables Template
# Copy this file to .env and fill in your actual values

# ============================================
# GOOGLE OAUTH CONFIGURATION
# ============================================
# Get this from Google Cloud Console:
# https://console.cloud.google.com/apis/credentials
GOOGLE_CLIENT_ID=your-google-client-id-here.apps.googleusercontent.com
VITE_GOOGLE_CLIENT_ID=your-google-client-id-here.apps.googleusercontent.com

# ============================================
# DATABASE CONFIGURATION
# ============================================
# MongoDB connection string
# Local: mongodb://localhost:27017/nw_it
# Atlas: mongodb+srv://username:password@cluster.mongodb.net/nw_it
MONGODB_URI=mongodb://localhost:27017/nw_it

# ============================================
# JWT CONFIGURATION
# ============================================
# Secret key for signing JWT tokens
# Generate a random string (min 32 characters)
JWT_SECRET=your-secure-random-secret-key-here-min-32-chars

# ============================================
# SERVER CONFIGURATION
# ============================================
# Port for backend server
PORT=5000

# Node environment
NODE_ENV=development

# Production URL (if applicable)
VITE_PRODUCTION_URL=https://yourapp.com

# ============================================
# CORS CONFIGURATION (Optional)
# ============================================
# Allowed origins for CORS (comma-separated)
CORS_ORIGIN=http://localhost:5173,http://localhost:3000

# ============================================
# PAYMENT GATEWAY (if applicable)
# ============================================
RAZORPAY_KEY_ID=your-razorpay-key-id
RAZORPAY_KEY_SECRET=your-razorpay-secret

# ============================================
# LOCAL LLM CONFIGURATION
# ============================================
# Provider can be 'ollama', 'lmstudio', or 'openai-compatible'
LLM_PROVIDER=ollama

# Model name (must be available on your LLM server)
# Examples: qwen2.5-coder:3b, llama3.1:8b, mistral, deepseek-coder-v2
LLM_MODEL=qwen2.5-coder:3b

# Ollama server URL (local or remote proxy)
# Local: http://localhost:11434
# Remote proxy example: https://your-ollama-proxy.railway.app
OLLAMA_URL=http://localhost:11434

# API key for authenticated Ollama proxies (optional for local, required for some remote deployments)
# This will be sent as Bearer token in Authorization header
OLLAMA_API_KEY=

# LM Studio base URL (OpenAI-compatible) - only needed if using LM Studio
LMSTUDIO_URL=http://localhost:1234/v1

# OpenAI-compatible endpoint URL (for generic OpenAI API compatible services)
# Only needed if LLM_PROVIDER=openai-compatible
OPENAI_COMPATIBLE_URL=
